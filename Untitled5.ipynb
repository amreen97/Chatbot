{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPiNOP+JFFe/9V1gON8MM8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amreen97/Chatbot/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "p7lZ7aDrnClw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "AAQP48-JnF4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "zlOacEwnnHTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Z8NKtmsYnHcp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK7Zg731mDaQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np # type: ignore\n",
        "import tensorflow as tf  # type: ignore\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "intents= json.loads(open('intents.json').read())\n",
        "\n",
        "words=[]\n",
        "classes=[]\n",
        "documents=[]\n",
        "ignoreletters=['?','!','.',',']\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        wordlist=nltk.word_tokenize(pattern)\n",
        "        words.extend(wordlist)\n",
        "        documents.append((wordlist,intent['tag']))\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "words=[lemmatizer.lemmatize(word) for word in words if word not in ignoreletters]\n",
        "words=sorted(set(classes))\n",
        "classes=sorted(set(classes))\n",
        "\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))\n",
        "\n",
        "training=[]\n",
        "outputempty=[0]*len(classes)\n",
        "\n",
        "for document in documents:\n",
        "    bag=[]\n",
        "    wordpatterns=document[0]\n",
        "    wordpatterns=[lemmatizer.lemmatize(word.lower()) for word in wordpatterns]\n",
        "    for word in words: bag.append(1) if word in wordpatterns else  bag.append(0)\n",
        "\n",
        "    outputrow=list(outputempty)\n",
        "    outputrow[classes.index(document[1])]=1\n",
        "    training.append(bag+outputrow)\n",
        "\n",
        "random.shuffle(training)\n",
        "training=np.array(training)\n",
        "\n",
        "trainx=training[:,:len(words)]\n",
        "trainy=training[:,len(words):]\n",
        "\n",
        "model=tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(128,input_shape=(len(trainx[0]),),activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(len(trainy[0],),activation='softmax'))\n",
        "\n",
        "sgd=tf.keras.optimizers.SGD(learning_rate=0.01,momentum=0.9,nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "hist=model.fit(np.array(trainx),np.array(trainy),epochs=200,batch_size=5,verbose=1)\n",
        "model.save('chatbot_mine.h5',hist)\n",
        "print('Executed')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "F5CzxoQwnHh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from keras.models import load_model\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "intents=json.loads(open('intents.json').read())\n",
        "words=pickle.load(open('words.pkl','rb'))\n",
        "classes=pickle.load(open('classes.pkl','rb'))\n",
        "\n",
        "model=load_model('chatbot_mine.h5')\n",
        "\n",
        "def clean_up_sentence(sentence):\n",
        "  sentence_words=nltk.word_tokenize(sentence)\n",
        "  sentence_words=[lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "  return sentence_words\n",
        "\n",
        "def bag_of_words(sentence):\n",
        "  sentence_words=clean_up_sentence(sentence)\n",
        "  bag=[0]*len(words)\n",
        "  for w in sentence_words:\n",
        "    for i,word in enumerate(words):\n",
        "      if word==w:\n",
        "        bag[i]=1\n",
        "  return np.array(bag)\n",
        "\n",
        "def predict_class(sentence):\n",
        "  bow=bag_of_words(sentence)\n",
        "  res=model.predict(np.array([bow]))[0]\n",
        "  ERROR_THRESHOLD=0.25\n",
        "  results=[[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "  results.sort(key=lambda x:x[1],reverse=True)\n",
        "  return_list=[]\n",
        "  for r in results:\n",
        "    return_list.append({'intent':classes[r[0]],'probability':str(r[1])})\n",
        "  return return_list\n",
        "\n",
        "\n",
        "def get_response(intents_list,intents_json):\n",
        "  tag=intents_list[0]['intent']\n",
        "  list_of_intents=intents_json['intents']\n",
        "  for i in list_of_intents:\n",
        "    if i['tag']==tag:\n",
        "      result=random.choice(i['responses'])\n",
        "      break\n",
        "\n",
        "\n",
        "  return result\n",
        "\n",
        "print('Great!Bot is runninng')\n",
        "\n",
        "while True:\n",
        "  message=input('')\n",
        "  ints=predict_class(message)\n",
        "  res=get_response(ints,intents)\n",
        "  print(res)"
      ],
      "metadata": {
        "id": "50V2t0w9MRIy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}